{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0nDg4mj4d0_"
      },
      "source": [
        "## Mount Google Drive (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NewXpFmp4icP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33UFsk6Z4bwq"
      },
      "source": [
        "## Clone the Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaK4Wxkn3WDZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/markuryy/paligemma-image-captioner.git\n",
        "%cd paligemma-image-captioner/finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq3HKFbt3zIi"
      },
      "source": [
        "## Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPjZzuyG34k2"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf88so0V36I1"
      },
      "source": [
        "## Download the Tokenizer and Big Vision Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfZce3Yv39Ib"
      },
      "outputs": [],
      "source": [
        "!python download_tokenizer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU2F2SlL71J3"
      },
      "source": [
        "## Convert to HF\n",
        "\n",
        "Colab T4 free tier doesn't have enough ram :(\n",
        "\n",
        "  System RAM peaks at ~19 GB, GPU RAM peaks at ~16 GB\n",
        "  \n",
        "L4 is recommended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvworMwK7z2J"
      },
      "outputs": [],
      "source": [
        "!python convert_to_hf.py --checkpoint_path /content/drive/MyDrive/models/my-custom-paligemma-ckpt-final.npz --tokenizer_model_file ./paligemma_tokenizer.model --pytorch_dump_folder_path /content/drive/MyDrive/models/Paligemma/finetune1_hf --precision float32 --variant \"3b-448px\" --do_convert_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfzRJ6IRCk72"
      },
      "source": [
        "## Push to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSav2y6XEakw"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import HfApi, HfFolder, Repository\n",
        "\n",
        "# Log in to Hugging Face\n",
        "import os\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHdy8fF_CnkG"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "# Set your variables\n",
        "model_name = \"markury/paligemma-448-ft-1\"  # Replace with your Hugging Face username and desired model name\n",
        "model_path = \"/content/drive/MyDrive/models/Paligemma/finetune1_hf\"  # Path where the model is saved\n",
        "\n",
        "# Create a model repository if it doesn't exist\n",
        "create_repo(model_name, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Upload the model to the repository\n",
        "upload_folder(\n",
        "    folder_path=model_path,\n",
        "    repo_id=model_name,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"convert npz to safetensors\"\n",
        ")\n",
        "\n",
        "print(f\"Model pushed to Hugging Face Hub under {model_name}!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
